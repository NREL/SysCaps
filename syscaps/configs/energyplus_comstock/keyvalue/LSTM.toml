[model]
attribute_encoder_type = 'text'
seq_type = 'rnn'
seq_input_dim = 103
seq_hidden_size = 512
seq_num_layers = 1
onehot_attr_input_dim = 336
mlp_hidden_dim = 256
qoi_dim = 1
text_encoder_name = 'distilbert-base-uncased'
text_freeze_encoder = false
continuous_head = 'mse'
ignore_attributes = false
timestamp_embedding_size = 32

[experiment]
module_name = 'sequential'
lr = 0.0003
batch_size = 32 
max_train_steps = 200000